{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the neighborhood of song recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import importlib\n",
    "\n",
    "import mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will allow us to embed images in the notebook\n",
    "%matplotlib inline\n",
    "# change default plot size\n",
    "plt.rcParams['figure.figsize'] = (15,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prep data\n",
    "\n",
    "* Load the full data set\n",
    "* Vectorize the playlists into sparse vectors\n",
    "* Extract the vocabulary with tid to allow translation back to track_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd_all=mpd.load(spark, \"onebig\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from mpd with each playlist being a sparse vector of tracks.  The vector index is the popularity of the song in the global playlist collection with the index value being the appearance count of the track in the playlist.\n",
    "\n",
    "This count is ignored and treated as 1.0 in track-based playlist searches because the goal is simply to find playlists that share common tracks.  That is, we treat the playlists as the user matrix with each track as a record of a \"purchased\" item.\n",
    "\n",
    "The vectorized playlist are limited to the 2^18 most popular songs, the default for CountVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, result = mpd.vectorizecol(mpd_all.select(\"pid\", \"tracks.track_uri\"), \"track_uri\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdf = mpd.buildvocabdf(spark, model.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate mapping of tracks in voabulary to human readable names from mpd dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "tname=mpd_all.select(explode(\"tracks\").alias(\"tracks\")).select(\"tracks.track_name\", \"tracks.track_uri\", \"tracks.artist_name\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdf.join(tname, tname.track_uri == vdf.term).drop(vdf.term).orderBy(\"tid\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for kNN search\n",
    "\n",
    "* Prep dataset with vector length to elliminate empty playlists from minHash input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "vectorlength = udf(lambda x: x.numNonzeros(), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2=result\n",
    "r2=r2.withColumn(\"vlen\", vectorlength(r2.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider only playlists with one or more entries because MinHash can't handle empty vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsevec = r2.where(r2.vlen > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample only 10% of the mpd to build the characteristic matrix for the initial search space in consideration of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsevec= sparsevec.rdd.sample(False, .01, 1).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the signature matrix with min hashing.  Each signature has 5 hashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mh.fit(sparsevec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformA = model.transform(sparsevec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signature vectors are the hashes column.  Together with the playlist id they make up the ~1,000,000x5 signature matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore results for full playlist match\n",
    "\n",
    "Grab the first playlist in the mpd as a test list (it is very unlikely to be in the sampled set above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpl=result.select(\"features\").rdd.map(lambda x: x.features).take(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 100 nearest neighbors to get enough playlists to ensure we have 500 tracks to recommend.  On average, 100 playlists should contain about 660 tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Approximately searching dfA for 100 nearest neighbors of the key:\")\n",
    "k100nn = model.approxNearestNeighbors(transformA, testpl, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distribution of distances in the 100 neighbors.  Most are far away but some are pretty close.  Given we are only getting 100 near playlists we are naively dictating that our cluster size is 100 simply to make sure we have enough tracks in teh cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd.plothist(k100nn, \"distCol\", 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the popularity of tracks in knn cluster playlists\n",
    "\n",
    "This follows the inverse decay shape of global track/artist popularity.\n",
    "\n",
    "It seems logical to assume that track popularity within similar playlists is a good (if basic) foundation for recommended tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nntracks=k100nn.select(explode(\"track_uri\").alias(\"track_uri\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackrank = k100nntracks.select(\"track_uri\").groupby(\"track_uri\").count().sort(f.col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackrank.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd.scatterplotfreq(trackrank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore results for a subset of playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse vectors have an [indices method](http://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector.indices) to get the array elements with values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpl.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a subset of the playlist is easy by looking at the indices\n",
    "They are returned as a numpy array. \n",
    "[numpy has a built in to chose a random sample from an array](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand5npa = np.random.choice(testpl.indices, 5, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the machine learning libararies expect sparse vectors of the new ml.linalg package not the mllib.linalg package.  There is a [conversion method for the old format to return as the new ML format](http://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector.asML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(rand5npa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testpl.size, np.sort(rand5npa), np.ones(len(rand5npa)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use np.sort and np.ones to help create new spase vector for the search.\n",
    "\n",
    "The sampled playlist still exists in the dimensionality of the of the original testpl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand5pl = SparseVector(testpl.size, np.sort(rand5npa), np.ones(len(rand5npa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand5pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for the k=100 neighbors for the 5-element playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn5seed = model.approxNearestNeighbors(transformA, rand5pl, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn5seed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we're only getting 5 candidates so that must mean we [don't have enough candidates in the hash bucket](https://spark.apache.org/docs/2.2.0/ml-features.html#approximate-nearest-neighbor-search).  Wonder if increasing the hash number in the minhash will improve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd.plothist(k100nn5seed, \"distCol\", 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore impact of choice on construction of sampled playlist\n",
    "\n",
    "I would have expected more results not less from a smaller playlist.\n",
    "I'm assuming that a smaller set of tracks would match a larger collection of playlists.\n",
    "\n",
    "This could be a luck of the draw for the random selection. Repeating the random choice selection from the playlist now gives me 11 results.\n",
    "\n",
    "So this is challenging. How to ensure I get enough results from the search\n",
    "\n",
    "I assume what is happening is that if there are tracks that are less popular in the search then I'm going to be more of an edge case playlist.\n",
    "\n",
    "Yeah, and if i take just the top-5 most popular songs in the playlist then i get my 100 results as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly choose tracks from playlist. Leads to small result set for near neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand5npa = np.random.choice(testpl.indices, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose 5 most popular songs from playlist. Leads to high number of near neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand5npa = np.sort(testpl.indices)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the first 5 songs from the playlist. This is the continuation model.  You hear 5 songs and now recommend the rest.\n",
    "\n",
    "Note, in the current construction of testpl we have lost the orignal playlist order since we are deriving this from the sparse vector.\n",
    "\n",
    "Really need to go back and redefine testpl as coming from the original playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand5npa = testpl.indices[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand5pl = SparseVector(testpl.size, np.sort(rand5npa), np.ones(len(rand5npa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand5pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longer signature matrix doesn't create larger clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh10 = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = mh10.fit(sparsevec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformA10 = model10.transform(sparsevec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This search is with a top 5 tracks selected sample playlist and a larger signature matrix.  The higher hit count of neighbors comes from the more popular track selection for the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn5seed10 = model.approxNearestNeighbors(transformA10, rand5pl, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn5seed10.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have more neighbors with the same small search playlist, most of those neigbors are distant. This makes sense because we are looking for similarity on a small cross section of each playlist.\n",
    "\n",
    "This is somewhat akin to the banding method for speeding LSH with minhash signature matrix.  If our search term is a band then we have matched a number of playlists that share these tracks in that band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd.plothist(k100nn5seed10, \"distCol\", 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore artists as clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artists make an inherently good clustering since the naturally and logically shrink the collection of tracks.  They also provide the potential for developing a utility matrix with explicit ratings since the same artist is much more likely to appear multiple times in the same playlist, suggesting a preference for the artist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artist utility matrix is built from the original mpd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amodel, aresult = mpd.vectorizecol(mpd_all.select(\"pid\", \"tracks.artist_uri\"), \"artist_uri\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avdf = mpd.buildvocabdf(spark, amodel.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review artist names to get a sense of the most popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aname=mpd_all.select(explode(\"tracks\").alias(\"tracks\")).select(\"tracks.artist_uri\", \"tracks.artist_name\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avdf.join(aname, aname.artist_uri == avdf.term).drop(avdf.term).orderBy(\"tid\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artist-based playlists are also not likely to have empty vectors since they fall under the 2^18 default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar2=aresult\n",
    "ar2=ar2.withColumn(\"vlen\", vectorlength(ar2.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asparsevec = ar2.where(ar2.vlen > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asparsevec.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work with a 10% sample of the artist-based playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asparsevec= asparsevec.rdd.sample(False, .01, 1).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the signature matrix for artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amhmodel = amh.fit(asparsevec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amhtransform = amhmodel.transform(asparsevec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a sample search playlist for artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atestpl=aresult.select(\"features\").rdd.map(lambda x: x.features).take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atestpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for artists playlists similar to the search playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Approximately searching dfA for 100 nearest neighbors of the artist:\")\n",
    "ak100nn = model.approxNearestNeighbors(amhtransform, atestpl, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is much better similarity across artist based playlists.\n",
    "\n",
    "This distribution is simlar to when we use the whole playlist to search the playlist space.  This suggests that better coverage of the playlist and tracks will build a better neighbor hood with gradually increasing distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd.plothist(ak100nn, \"distCol\", 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the artist popularity in the k=100 neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nnartists=ak100nn.select(explode(\"artist_uri\").alias(\"artist_uri\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistrank = k100nnartists.select(\"artist_uri\").groupby(\"artist_uri\").count().sort(f.col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd.scatterplotfreq(artistrank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore subset of playlist match with artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atestpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get 5 elements from the artist search vector.  Again the most popular due to the construction of the sparse vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arand5npa = np.random.choice(atestpl.indices, 5, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arand5npa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arand5pl = SparseVector(atestpl.size, np.sort(arand5npa), np.ones(len(arand5npa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak100nn5seed = model.approxNearestNeighbors(amhtransform, arand5pl, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak100nn5seed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak100nn5seed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak100nn5seed.orderBy(\"pid\").orderBy(\"distCol\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the playlist name matches are less perfect than with the full playlist search, at least based on superficial name matching.\n",
    "\n",
    "Get the sense that searching for knn with the full sample playlist by tracks got many similarly named playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak100nn5seed.join(mpd_all.select(\"pid\", \"name\"), mpd_all.pid == ak100nn5seed.pid).drop(mpd_all.pid).select(\"pid\", \"name\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak100ordered = ak100nn5seed.orderBy(\"pid\").orderBy(\"distCol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output of the matches sorted by distance (in contract to the random result order above) does give a little better match to the theme of \"throwback\" even though the words are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak100ordered.join(mpd_all.select(\"pid\", \"name\"), mpd_all.pid == ak100ordered.pid).drop(mpd_all.pid).select(\"pid\", \"name\", \"distCol\").orderBy(\"distCol\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore knn with full vocab and sample playist\n",
    "\n",
    "See how it works to search against a larger playlist database.  Do small samples of just 5 songs return larger neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, result = mpd.vectorizecol(mpd_all.select(\"pid\", \"tracks.track_uri\"), \"track_uri\", \"features\", size=1188873 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhall = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelall = mhall.fit(sparsevec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformall = modelall.transform(sparsevec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn5seedall = modelall.approxNearestNeighbors(transformall, rand5pl, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn5seedall.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the full playlist collection of all songs that have more than one occurance the neighborhood results are much better. The search performance doesn't seem to have changed much either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd.plothist(k100nn5seedall, \"distCol\", 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn5seedall.join(mpd_all.select(\"pid\", \"name\"), mpd_all.pid == k100nn5seedall.pid).drop(mpd_all.pid).select(\"pid\", \"name\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn5seedall.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn5seedall = k100nn5seedall.withColumnRenamed(\"pid\", \"recpid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn5seedall.join(mpd_all.select(\"pid\", \"name\"), mpd_all.pid == k100nn5seedall.recpid).drop(mpd_all.pid).select(\"recpid\", \"name\", \"distCol\").orderBy(\"distCol\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore quality of music recommendation\n",
    "\n",
    "Get top 500 recommended songs from 100 recommended playlists based on 5 tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn5seedall.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the ranked resutls of tracks from the recommended neighboring playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackrank=k100nn5seedall.select(explode(k100nn5seedall.track_uri).alias(\"track_uri\")).groupBy(\"track_uri\").count().sort(f.col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackrank.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=trackrank.select(\"count\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame({'X': range(1,Y.size+1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get global track info from knn recommended neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k100nn5seedall = k100nn5seedall.withColumnRenamed(\"pid\", \"recpid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pln=k100nn5seedall.join(mpd_all, mpd_all.pid == k100nn5seedall.recpid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pln.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pln.select(\"name\", \"modified_at\", \"num_edits\", \"num_followers\",\"num_tracks\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDF=pln.select(\"pid\", explode(\"tracks\").alias(\"track\")).select(\"track.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDF.select(\"track_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get knn playlist and global ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the global track vocabulary, where the track id (tid) is the rank of the song in the global dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvdf = mpd.buildvocabdf(spark, model.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the global track rank column (track id) from the global track vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grank=trackrank.join(tvdf, trackrank.track_uri == tvdf.term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grank.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grank.orderBy(\"count\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grank.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendrank=grank.orderBy(f.desc(\"count\"), f.desc(\"tid\")).select(\"track_uri\", \"count\", \"tid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendrank.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a row index in recommendation order.  \n",
    "\n",
    "Can't use monotonically increasing by itself because it skips numbers. Need to [convert to a an increase by one id](https://stackoverflow.com/a/48211877). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendrank=recommendrank.withColumn(\"mid\", f.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = W.orderBy(\"mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendrank = recommendrank.withColumn(\"rank\", f.row_number().over(windowSpec)).drop(\"mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendrank.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendrank.describe(\"rank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand5pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Convert a numpy array to a dataframe via a pandas dataframe](https://stackoverflow.com/a/46818308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand5pl.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rand5pl.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r5pldf = spark.createDataFrame(pd.DataFrame(rand5pl.indices), [\"idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r5pldf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search tracks make up half the top 10 popularity of the knn trackrank results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendrank.join(r5pldf, recommendrank.tid == r5pldf.idx).drop(r5pldf.idx).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to see the intersection of the recommended tracks and the tracks that were held out of the search playlist.\n",
    "\n",
    "This is hard to express in sql because a not-equal between columns doesn't reflect the semantics of not in.  That just elliminates the search playlist tracks from the recommended result.\n",
    "\n",
    "`recommendrank.join(r5pldf, recommendrank.tid != r5pldf.idx)`\n",
    "\n",
    "It might be quick to eliminate the search tracks from the full playlist with a dot product?  No cause that sums them up.  What i really want is a mask.  Feel this should be easier with sql.\n",
    "\n",
    "There is a [not in clause with negation ~](https://stackoverflow.com/a/40607572)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendrank.where(~recommendrank.tid.isin(rand5pl.indices.tolist())).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtracting the search playlist from full playlist should get me an index I can intersect with the ranked recommended tracks and see how the real playlist compares the recommended tracks.\n",
    "\n",
    "This would work but mainly need to get the testpl in a dataframe and then do a not in join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plidx = spark.createDataFrame(pd.DataFrame(testpl.indices), [\"idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plidx.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recidx = plidx.where(~plidx.idx.isin(rand5pl.indices.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(recidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recidx.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recidx.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So of the remaining tracks in the playlist only about 2/3 are covered in the top 500 continuation track recommendations. This means the recommendation model based on popularity isn't great.  Also we missing about 11 tracks that didn't show up recommended tracks at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remainingrank = recommendrank.join(recidx, recidx.idx == recommendrank.tid).drop(recidx.idx).orderBy(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remainingrank.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remainingrank.show(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd.plothist(remainingrank, \"rank\", 11)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
