{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import spark, data and local libraries\n",
    "\n",
    "Pyspark functions provide many utilies to process data in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will allow us to embed images in the notebook\n",
    "%matplotlib inline\n",
    "# change default plot size\n",
    "plt.rcParams['figure.figsize'] = (15,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build [spark connection](https://spark.apache.org/docs/latest/sql-programming-guide.html#starting-point-sparksession) to running cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a spark context to understand and control heap space.  Create it from the spark session already created above.\n",
    "https://stackoverflow.com/a/41267415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "#conf = SparkConf().setAppName(\"mpd\").setMaster(\"spark://localhost:\")\n",
    "#sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to get the information about driver memory to avoid heap errors\n",
    "https://datascience.stackexchange.com/a/12191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sc._conf.get('spark.driver.memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can [get the full configuration of the spark context](https://stackoverflow.com/a/30564947).\n",
    "\n",
    "This shows the attribute has changed from spark.driver.memory to spark.executor.memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#conf = SparkConf().setAppName(\"Python Spark SQL basic example\")\n",
    "#conf = (sc._conf.setMaster('spark://c0015:7077')\n",
    "        .set('spark.executor.memory', '4G')\n",
    "        .set('spark.driver.memory', '45G')\n",
    "        .set('spark.driver.maxResultSize', '10G'))\n",
    "#sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#conf = SparkConf().setAppName(\"Python Spark SQL basic example\")\n",
    "conf = (sc._conf\n",
    "        .set('spark.executor.memory', '4G')\n",
    "        .set('spark.driver.memory', '45G')\n",
    "        .set('spark.driver.maxResultSize', '10G'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the mpd data set and run basic count santity check and print the schema for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd_all=mpd.load(spark, \"onebig\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd_all.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore power law of two variables\n",
    "\n",
    "The power law is a linear relationship between the logarithms of variables. [Leskovec2014, 1.3.6]\n",
    "A good blog on [power law characteristics and the type of power laws to expect](https://www.fs.blog/2017/11/power-laws/)\n",
    "\n",
    "Look at the top N artists, albums, songs and observe the power law relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to count tracks by uri but the following appproach doesn't work.  It's selects all the track_uris for each playlist as a string, not as individual (countable) tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toptracks=mpd_all.select(\"tracks.track_uri\").groupby(\"track_uri\").count().sort(f.col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toptracks.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#toptracks.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one works because the playlist is exploded into a two column pid and track_uri, which can be grouped and counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=mpd.playlist_flatten(mpd_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The track count matches the values from the stats.txt distributed with the mpd data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackrank = pdf.select(\"track_uri\").groupby(\"track_uri\").count().sort(f.col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackrank.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trackrank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trackrank.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an X range to match the length of ranked tracks.\n",
    "Capture the track rank as a local pandas data frame to speed plotting.\n",
    "Use the .toPandas() method to get data into pandas format for plotting and make it easy to work with multiple replots without having to recompute on the spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=trackrank.select(\"count\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Y.size' matches the unique track count in the MPD, so we are seeing the rank of all the expected tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame({'X': range(1,Y.size+1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot has an extreme hook which doesn't match the power law shape expected.\n",
    "\n",
    "Inspecting the data range more closely we see the same basic hook in the first 10K songs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(X.head(10000),Y.head(10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But looking at the next 10K, we see the characteristic power law shape.  The Popularity of songs drops linearly in the log-log graph.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global shape has a superlinear (faster than linear) drop off in the first 10K.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the next 1000 has a very linear decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(X[10000:11000],Y[10000:11000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding that to the next 10K shows the emerging power law shape again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(X[10000:20000],Y[10000:20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, looking at the remaining data set of 10K and above and we see the hook shape again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(X[10000:X.size],Y[10000:Y.size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shape is most promentent in the 10k to 500k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(X[10000:500000],Y[10000:500000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it continues on until even the very end when we see the popular of tracks drop to one across a large portion of the playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(X[500000:X.size],Y[500000:Y.size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the tracks that appear just once in all play lists make up almost half of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[Y['count'] < 2].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost half the tracks appear no more than once in the entire collection of playlist.   These songs are not likely to be predictive, unless our intention is to promote least frequently heard songs i.e. promote discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[Y['count'] < 2].count() / Y.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[Y['count'] >= 2].count() + Y[Y['count'] < 2].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(X.head(100),Y.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistrank = pdf.select(\"artist_uri\").groupby(\"artist_uri\").count().sort(f.col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aY=trackrank.select(\"count\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aX=pd.DataFrame({'X': range(1,aY.size+1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(aX,aY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(aX.head(100),aY.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loglog Power Law Graphs\n",
    "Convert loglog plots to dots instead of lines to avoid continuous interpretations.\n",
    "\n",
    "Inspecting the log-log graphs should show us linear relationships if this is the standard power law, Y=mX^B.  This has been confusing because we see a shape that the more X increases (less popular song) the more Y decreases.  According to [the power-law blog](https://www.fs.blog/2017/11/power-laws/) this look like like Y=mX^-2 the inverse square law: the further we are from popular the less popular we will become.  That is, the less likely a track is to be included in a playlist.\n",
    "\n",
    "From the data above, only about half the tracks are predictave (appear more than once) and of those only about the top 10K appear with any frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.loglog(X.head(1000),Y.head(1000), linestyle=\"None\", marker=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.loglog(X,Y, linestyle=\"None\", marker=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.loglog(X.head(100),Y.head(100), linestyle=\"None\", marker=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.loglog(X.head(10000),Y.head(10000), linestyle=\"None\", marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artists\n",
    "\n",
    "Artists follow the same shape in their powerlaw plot.  The less popular an artist the ever increasing unpopularity they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.loglog(aX,aY, linestyle=\"None\", marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "build a playlist summary of all track uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDF=mpd_all.select(\"pid\", \"tracks.track_uri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Ranked 2-ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"track_uri\", outputCol=\"ngrams\")                                \n",
    "ngramdf = ngram.transform(pDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngramdf.select(\"ngrams\").show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "represent 2grams as an exloded liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2list = ngramdf.select(\"pid\", f.explode(\"ngrams\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2list.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the count of 2-ngrams is exactly 1-million less (number of playlists) than the number of songs because each playlist is the source of ngrams.  Each playlist will be missing 1 2-ngram per playlist. The final song in the playlist won't serve the start of an ngram.\n",
    "\n",
    "This indicates that a 3-ngram count would be 2 million less because each two last songs in a playlist would not serve as the start of an ngram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2cnt = ngram2list.count()\n",
    "ngram2cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2rank = ngram2list.select(\"col\").groupby(\"col\").count().sort(f.col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2rank.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngram2rank.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2rankcnt = ngram2rank.count()\n",
    "ngram2rankcnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are almost half as many unique 2 ngrams as their are songs, so 2 ngrams are not uncommon.\n",
    "\n",
    "Need to understand their popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ranked the ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the  2-gram rank we need to get a sense of its shape.\n",
    "\n",
    "There are  too many data point to plot meaningfully.  Practically, trying to return 36-million data elments into pandas (as we did above with the 2-million) won't work. It's guarunteed to return Java GC and heap errors.\n",
    "\n",
    "We can look at a subset of the data by sampling down to a small fraction of the data in our cluster.  Here we convert to RDD and then run the sample function to get a small number of points 360k and plot those.\n",
    "\n",
    "Note the rdd.sample() function returns an rdd so it can easily be converted to a spark dataframe and then brought into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nYsample=ngram2rank.select(\"count\").rdd.sample(False, .01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nYsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nY=nYsample.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nY.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nY=nY.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nY.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nX=pd.DataFrame({'X': range(1,nY.size+1,1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot shows the steep falloff and we're out of popular ngrams in the first few hundred.\n",
    "\n",
    "In other words, most 2grams are unique.  This suggests they won't make a very good predictor of what's next.  At least not in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(nX,nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the log-log plot shows a much more linear shape and intuitively much closer to the power law.\n",
    "\n",
    "The shape is also very simlar to the popularity of song and artist.  There is definitely a Top-100 feel to all these rankings.  All the rest just decay in popularity at the same rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.loglog(nX,nY, linestyle=\"None\", marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-log plot suggests we want to know the popularty of the different ngram counts.  There are a large number of ngrams that occur very infequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the top 10k\n",
    "\n",
    "We want to take the Top-10k (sorted by popularity above) and plot them directly.  This will let us take a good look at the first part of the curve. Give that we sampled out the data to produce the above graphs we'll get a better sense of a meaninful shape to the cuve and some real popularity numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nY1k=ngram2rank.select(\"count\").rdd.take(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nY1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nY1k=pd.DataFrame(nY1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nX1k=pd.DataFrame({'X': range(1,nY1k.size+1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(nX1k,nY1k, linestyle=\"None\", marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under this curve is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nY1k.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nY1k.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about 10% of the ngrams that are have a popularity over about 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nY1k.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.loglog(nX1k,nY1k, linestyle=\"None\", marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the popularity of the Top-10k 2grams ranges from 2500 to about 200.  \n",
    "\n",
    "These seem like pretty small number for a count of 36-million"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the ranked popularity\n",
    "\n",
    "How many 2-ngrams occur less than 200 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2rank.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the count column, which is actually a transformation method, to a an actual column of data representing real values.\n",
    "\n",
    "Err.\n",
    "\n",
    "Now realizing i thought this was a method because of the way i was calling it.  type(df.count) is a method because it is part of the data frame class.  type(df.select(\"count\")) is a dataframe column because the word count doen't get mis-parsed.\n",
    "\n",
    "The errors below may simply be parsing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrdd=ngram2rank.select(\"count\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nrdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf=nrdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ndf.select(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually [several ways of renaming a column](https://stackoverflow.com/a/34077809).  The selectExpr() is easiest to use a sql-like syntax but if I want to use alias, just need to cast a string as a column first to use the alias method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntcol=ndf.selectExpr(\"count as rcount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cntcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntcol.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntcol=ndf.select(f.col(\"count\").alias(\"rcount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cntcol.rcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2poprank = ngram2rank.selectExpr(\"count as rcount\").groupby(\"rcount\").count().sort(f.col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2poprank.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1202 distinct 2-ngram rankings.  I've taken away the ngram association so all ngrams that appear once for example are treated as once-occuring-2-ngrams an counted in that group.  Likewise for twice occuring 2-ngrams.  This continutes on to the many-times occurring 2-ngrams, the most popular 2-ngrams.  These will have fewer occuring ngrams.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2poprank.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ngram2poprank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2poprank.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that 30mill of the 35mill 2-ngrams appear only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2poprank[\"rcount\" == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankY=ngram2poprank.select(\"count\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankY.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankX=pd.DataFrame({'X': range(1,rankY.size+1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.scatter(rankX, rankY, linestyle=\"None\", marker=\".\")\n",
    "plt.pyplot.xlabel(\"ngram appears x-times\")\n",
    "plt.pyplot.ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.semilogy(rankX[0:200], rankY[0:200], linestyle=\"None\", marker=\".\")\n",
    "plt.pyplot.xlabel(\"ngram appears x-times\")\n",
    "plt.pyplot.ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.loglog(rankX, rankY, linestyle=\"None\", marker=\".\")\n",
    "plt.pyplot.xlabel(\"ngram appears x-times\")\n",
    "plt.pyplot.ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loglog plot helps identify the popularity of different \"appearance count\" buckets.  The vast majority of 2-ngrams (over 30million out of 35million) fall into the \"appears only once\" bucket.  An additional 4 million 2-ngrams fall into the \"appear two to three times\".  The remaing 1-million appear four or more times.  The count of unique bucket types seems to grow pretty smoothly, each appearance count increasing by one.  Then there are several hundred buckets with a uniqe appearance count.\n",
    "\n",
    "Here it would be good to see a histogram of appearance count, to chunk the buckets, but expect it to be a steep drop off like the raw scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.hist(rankX)\n",
    "plt.pyplot.xlabel(\"ngram appears x-times\")\n",
    "plt.pyplot.ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this plot does not make much sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonder how far appart the bucket counts are and at what point we start getting further than a 1 increase in popularity.\n",
    "\n",
    "Look at tail of ranked popularity requires reverse sort because [spark doesn't have tail function yet](https://medium.com/@chris_bour/6-differences-between-pandas-and-spark-dataframes-1380cec394d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram2poprankrev = ngram2rank.selectExpr(\"count as rcount\").groupby(\"rcount\").count().sort(f.col(\"rcount\").desc())\n",
    "ngram2poprankrev.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Pipelines with CountVectorizer()\n",
    "\n",
    "Use CountVectorizer to count the ngrams that have min support of 2 and  build a vocabolary ranked by popularity.\n",
    "\n",
    "This is similar to the work above but just exploring pipelines and higherlevel functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"ngrams\", outputCol=\"features\", minDF=2)\n",
    "model = cv.fit(ngramdf)\n",
    "result = model.transform(ngramdf)      \n",
    "result.select(\"features\").show(1,truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocab size given in the feature vector by CountVectorizer matches the count in the model vocabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(model.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the most popular 2gram in the vocabulary by simply printing by its index in teh array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.vocabulary[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a flatmap to the count the terms\n",
    "https://mingchen0919.github.io/learning-apache-spark/tf-idf.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql.types import StringType\n",
    "df_vocab = result.select('ngrams').rdd.\\\n",
    "           flatMap(lambda x: x[0]).\\\n",
    "            toDF(schema=StringType()).toDF('terms')\n",
    "df_vocab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_vocab.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate term frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab_freq = df_vocab.rdd.countByValue()\n",
    "#pdf = pd.DataFrame({\n",
    "#        'term': vocab_freq.keys(),\n",
    "#        'frequency': vocab_freq.values()\n",
    "#    })\n",
    "tf = spark.createDataFrame(pdf).orderBy('frequency', ascending=False)\n",
    "#tf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_vocab.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#import org.apache.spark.sql.functions.monotonicallyIncreasingId \n",
    "newDf = df_vocab.withColumn(\"uniqueIdColumn\", f.monotonically_increasing_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab_freq = df_vocab.rdd.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandasdf = pd.DataFrame({\n",
    "        'term': vocab_freq.keys(),\n",
    "        'frequency': vocab_freq.values()\n",
    "    })\n",
    "tf = spark.createDataFrame(pdf).orderBy('frequency', ascending=False)\n",
    "tf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngram2Y=ngram2rank.select(\"count\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X=pd.DataFrame({'X': range(1,ngram2Y.size+1,1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Numeric Representations\n",
    "\n",
    "This [post recommends the dataframe functions for indexstring](https://stackoverflow.com/a/43971119)\n",
    "\n",
    "The functions [IndexToString and StringIndexer](https://spark.apache.org/docs/2.2.0/ml-features.html#indextostring) convert strings to numeric format for ML training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDF=pdf.select(\"pid\", \"track_uri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDF=pDF.limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"track_uri\", outputCol=\"trackID\")\n",
    "model = indexer.fit(pDF)\n",
    "indexed = model.transform(pDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed string column '%s' to indexed column '%s'\"\n",
    "      % (indexer.getInputCol(), indexer.getOutputCol()))\n",
    "indexed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed.rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"StringIndexer will store labels in output column metadata\\n\")\n",
    "\n",
    "converter = IndexToString(inputCol=\"trackID\", outputCol=\"orig_track_uri\")\n",
    "converted = converter.transform(indexed)\n",
    "\n",
    "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\n",
    "      \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
    "\n",
    "converted.printSchema()\n",
    "#converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
